[ { "title": "ZFS Mirror, Snapshots, Recovery", "url": "/posts/basic-zfs/", "categories": "zfs, mirror, recovery, ubuntu", "tags": "zfs, mirror, recovery, ubuntu", "date": "2024-02-24 15:00:00 +0000", "snippet": "ZFS Configuration and Management GuideThis document outlines essential steps and considerations for configuring and managing ZFS storage pools, including creating a mirrored pool, setting properties, monitoring, and utilizing snapshots for data protection.Creating a ZFS PoolTo create a mirrored ZFS pool named new-pool with two disks (/dev/sdb and /dev/sdc), use the following command:sudo zpool create new-pool mirror /dev/sdb /dev/sdcCreating ZFS FilesystemsCreate a ZFS filesystem within your pool for organized data management:sudo zfs create new-pool/myfilesystemSetting ZFS PropertiesZFS supports various properties for optimizing storage. Examples include: CompressionTo save space, you can enable compression on your pool or on individual filesystems within the pool. sudo zfs set compression=on new-pool DeduplicationIf you expect to store a lot of duplicate data, enabling deduplication can save space, but be cautious as it can be very memory intensive. sudo zfs set dedup=on new-pool SnapshotsUtilize snapshots for data protection and point-in-time recovery:sudo zfs snapshot new-pool/myfilesystem@mySnapshotMonitoring Pool HealthRegularly check your ZFS pool’s health:sudo zpool status new-poolAutmating ScrubsScheduling regular scrubs of the pool can help detect and correct any data integrity issues. Scrubs can be scheduled using cron jobs# Example cron job to scrub on the first day of every month at midnight0 0 1 * * /usr/sbin/zpool scrub new-poolAutomating Snapshots with zfs-auto-snapshotInstall zfs-auto-snapshot for automatic snapshot management:sudo apt-get install zfs-auto-snapshotAfter installation, it automatically schedules snapshots and manages their retention.Customizing Snapshot PoliciesEdit cron jobs in /etc/cron.d to customize snapshot frequencies and retention policies.Manual Snapshot CreationCreate and manage snapshots manually:sudo zfs-auto-snapshot --keep=7 dailyExcluding Filesystems from SnapshotsExclude specific filesystems from automatic snapshots:sudo zfs set com.sun:auto-snapshot=false new-pool/myfilesystemRecovering from a Drive FailureWhen a drive fails in a ZFS pool, it’s crucial to replace it as soon as possible to maintain data redundancy and pool health. Here’s a step-by-step guide to recover from a drive failure: Identify the Failed Drive Use zpool status to identify the failed drive in your pool: sudo zpool status new-pool Offline the Failed Drive If the drive is still online but faulty, offline it manually: sudo zpool offline new-pool /dev/sdx Replace /dev/sdx with the actual device identifier of the failed drive. Replace the Drive Physically replace the failed drive with a new one. Ensure the new drive is of equal or greater capacity. Add the New Drive to the Pool Use the zpool replace command to replace the failed drive with the new one in your pool: sudo zpool replace new-pool /dev/sdx /dev/sdy Replace /dev/sdx with the failed drive and /dev/sdy with the new drive. Monitor Resilvering Process ZFS will begin the resilvering process to restore data redundancy. Monitor the progress with: sudo zpool status new-pool Verify Pool Health Once resilvering is complete, verify the pool’s health: sudo zpool status new-pool Ensure the pool status is ONLINE and there are no remaining errors. Setting Up RAID-Z (Equivalent to RAID 5)RAID-Z is ZFS’s equivalent to RAID 5, offering a balance between storage capacity, fault tolerance, and performance. Here’s how to set up a RAID-Z pool: Creating a RAID-Z Pool To create a RAID-Z pool, you need at least three disks. Here’s the command to create a RAID-Z pool named raidz-pool with three disks: sudo zpool create raidz-pool raidz /dev/sda /dev/sdb /dev/sdc Replace /dev/sda, /dev/sdb, and /dev/sdc with your actual disk identifiers. Checking Pool Status After creation, check the status of your pool to ensure it’s online and no errors are reported: sudo zpool status raidz-pool Expanding a RAID-Z Pool Expanding a RAID-Z pool can be done by adding another set of disks in a RAID-Z configuration. For example, to add another three disks to the pool: sudo zpool add raidz-pool raidz /dev/sdd /dev/sde /dev/sdf Note: ZFS does not support expanding an existing RAID-Z by adding a single disk to the vdev. Expansion typically involves adding a new vdev of the same type to the pool. Monitoring and Managing the Pool Use zpool status, zfs list, and other ZFS commands to monitor and manage your RAID-Z pool. Summary Creating a ZFS Pool: The guide starts with instructions on creating a mirrored ZFS pool using zpool create, demonstrating how to mirror two devices for redundancy. Setting Pool Properties: It explains how to adjust ZFS pool properties, such as setting the auto-snapshot feature using zfs set to automatically manage snapshots for backup and recovery purposes. Snapshot Management: The guide covers creating, listing, and rolling back snapshots, providing a foundation for efficient data management and recovery strategies. ZFS Auto-Snapshot: It introduces the zfs-auto-snapshot feature, which automates the snapshot process, and briefly touches on its configuration options for tailoring snapshot frequency and retention. Recovering from a Drive Failure: This section details the steps to identify and replace a failed drive in a ZFS pool, including offlining the failed drive, physically replacing it, and the subsequent resilvering process to restore data redundancy. Setting Up RAID-Z: Finally, the guide discusses setting up RAID-Z, ZFS’s equivalent to RAID 5, including creating a RAID-Z pool for improved fault tolerance and storage efficiency, and expanding a RAID-Z pool by adding additional sets of disks. Throughout, the guide emphasizes commands and procedures critical for managing ZFS pools and datasets, offering practical advice for ensuring data integrity, managing storage efficiently, and recovering from hardware failures." }, { "title": "Setting up a Custom Neovim config on Arch Linux", "url": "/posts/neovim-customization/", "categories": "Arch Linux, neovim, config, python, language support", "tags": "Arch Linux, neovim, config, python, language support", "date": "2024-02-14 11:00:00 +0000", "snippet": "vim-plugFirst we need to install (place the plug.vim into the autoload directory) vim-plug.curl -fLo ~/.local/share/nvim/site/autoload/plug.vim --create-dirs https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vimInstall npm and diagnostic language serverssudo pacman -S npmsudo npm install -g pyright yaml-language-server vscode-json-languageserver bash-language-server typescript typescript-language-server vscode-html-languageserver-bin vscode-css-languageserver-binNeovim commandsOk, we can update, and install specific packages, but using our config with TSInstall by itself you will install support for 200+ languages at the time of writing. I’ll put the commands here for explicitly installing python.:PlugInstall # installs plugins:TSInstall python # installs python:TSUpdate # updates all installed language parsers to their latest versions.Neovim configLocated $HOME/.config/nvim/init.vimcall plug#begin('~/.config/nvim/plugged')\" Your Plug commands will go here, for example:Plug 'nvim-treesitter/nvim-treesitter', {'do': ':TSUpdate'}Plug 'hrsh7th/nvim-cmp' \" The completion enginePlug 'hrsh7th/cmp-buffer' \" Buffer completionsPlug 'hrsh7th/cmp-path' \" Path completionsPlug 'hrsh7th/cmp-nvim-lsp' \" LSP completionsPlug 'hrsh7th/cmp-nvim-lua' \" Neovim Lua API completionsPlug 'neovim/nvim-lspconfig' \" Common configurations for the Neovim LSP clientPlug 'hrsh7th/cmp-vsnip' \" Snippet completionsPlug 'hrsh7th/vim-vsnip' \" Snippet enginecall plug#end()\" install programming language packslua &lt;&lt; EOFrequire'nvim-treesitter.configs'.setup { highlight = { enable = true, -- false will disable the whole extension }, ensure_installed = \"all\" -- one of \"all\", \"maintained\" (parsers with maintainers), or a list of languages}EOF\" Install for predicting typinglua &lt;&lt; EOFlocal cmp = require'cmp'cmp.setup({ snippet = { expand = function(args) vim.fn[\"vsnip#anonymous\"](args.body) -- For `vsnip` users. end, }, mapping = cmp.mapping.preset.insert({ ['&lt;C-b&gt;'] = cmp.mapping.scroll_docs(-4), ['&lt;C-f&gt;'] = cmp.mapping.scroll_docs(4), ['&lt;C-Space&gt;'] = cmp.mapping.complete(), ['&lt;C-e&gt;'] = cmp.mapping.abort(), ['&lt;CR&gt;'] = cmp.mapping.confirm({ select = true }), -- Accept currently selected item. Set `select` to `false` to only confirm explicitly selected items. }), sources = cmp.config.sources({ { name = 'nvim_lsp' }, { name = 'vsnip' }, -- For vsnip users. { name = 'buffer' }, { name = 'path' } })})EOF\" Powerful Diagnostic Functionalitylua &lt;&lt; EOF-- PYTHONrequire'lspconfig'.pyright.setup{}-- YAMLrequire'lspconfig'.yamlls.setup{}-- JSONrequire'lspconfig'.jsonls.setup{ commands = { Format = { function() vim.lsp.buf.range_formatting({}, {0,0}, {vim.fn.line(\"$\"),0}) end } }}-- Bashrequire'lspconfig'.bashls.setup{}-- JavaScript and TypeScriptrequire'lspconfig'.tsserver.setup{}-- HTMLrequire'lspconfig'.html.setup{}-- CSSrequire'lspconfig'.cssls.setup{}EOF\" toggle diagnosticslua &lt;&lt; EOFfunction ToggleDiagnostics() local current_value = vim.diagnostic.config().underline vim.diagnostic.config({underline = not current_value, virtual_text = not current_value, signs = not current_value, update_in_insert = not current_value})endEOF\" The Leader key defaults to \\. You can change it by uncommenting and setting the line below:\" let mapleader = \" \"nnoremap &lt;leader&gt;d :lua ToggleDiagnostics()&lt;CR&gt;\" Set tab to 4 spacesset tabstop=4set shiftwidth=4set expandtab\" Always show line numbersset number\" Toggle relative line numbers with F7nnoremap &lt;F7&gt; :if &amp;relativenumber == 0 \\| set relativenumber \\| else \\| set norelativenumber \\| endif&lt;CR&gt;" }, { "title": "Setting up an HP Printer on Arch Linux", "url": "/posts/arch-printer/", "categories": "Arch Linux, Printers, CUPS, HP", "tags": "HP, Printer, Arch Linux, CUPS", "date": "2024-02-10 13:00:00 +0000", "snippet": "Installing a printer on Linux can sometimes be a straightforward task, but it becomes interesting when you’re working with specific hardware and distributions. Today, I’m sharing my experience with setting up an HP Printer on Arch Linux, which required a slightly different approach than usual. This guide is tailored for the HP Envy 5000 series, but it may be helpful for other models and series as well.PrerequisitesBefore we dive into the installation process, make sure your system is up to date. It’s a good practice to update your package database and upgrade your system packages before installing new software.sudo pacman -SyuInstalling Required PackagesThe first step involves installing several packages that are necessary for printer setup and operation. While some of these packages might not be essential for everyone, they were useful during my diagnostic checks. Here’s what you need to install:sudo pacman -S python-pyqt5 xsane dbus python-pillow python-reportlab cups hplipEnabling and Starting CUPSCUPS (Common UNIX Printing System) is crucial for printing on Linux. After installing the necessary packages, you’ll need to enable and start the CUPS service:sudo systemctl daemon-reloadsudo systemctl enable --now cupsAdding the PrinterWith CUPS running, the next step is to add your printer to the system. The hp-setup command will guide you through finding and adding your HP printer:hp-setupVerifying the InstallationTo ensure that your printer has been installed correctly and is recognized by your system, run the following command:hp-info -iIf everything is configured correctly, you should now have a working HP printer on your Arch Linux system.ConclusionSetting up an HP printer on Arch Linux might require a few extra steps, but the process is straightforward once you know what packages are needed and how to enable the necessary services. I hope this guide helps you get your HP printer up and running smoothly on Arch Linux." }, { "title": "Nvidia GPU LXD Passthrough", "url": "/posts/jellyfin-nvidia-lxd/", "categories": "homelab, lxd, ubuntu, gpu, nvidia, jellyfin", "tags": "homelab, lxd, ubuntu, gpu, nvidia, jellyfin", "date": "2023-10-15 15:00:00 +0100", "snippet": "On the host…Install Nvidia driverssudo apt install nvidia-driver-535sudo rebootCheck this website, but the deb network install seemed to work best.Nvidia Cuda Install InstructionsAs of 15/10/2023 These work for ubuntu 22.04 (deb(network) install)wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.debsudo dpkg -i cuda-keyring_1.1-1_all.debsudo apt-get updatesudo apt-get -y install cudasudo rebootLaunch LXD Containerlxc launch ubuntu:22.04 jelly-belly -c nvidia.runtime=trueAdd GPU to containerlxc config device add jelly-belly gpu gpuAdd Extra Line to Configlxc config edit jelly-bellyInclude the top line alongside the runtime config.nvidia.driver.capabilities: allnvidia.runtime: \"true\"" }, { "title": "Software Raid Boot Drive Ubuntu", "url": "/posts/software-raid-boot/", "categories": "homelab, raid, mdadm, ubuntu, boot", "tags": "homelab, raid, mdadm, ubuntu, boot", "date": "2023-10-14 17:00:00 +0100", "snippet": "Ubuntu Software Raid on Boot DeviceOS: Ubuntu 22.04 serverAt Installation… Select “Custom storage layout” If the disks have existing partitions, click on each disk under AVAILABLE DEVICES and then select REFORMAT. Now select the 1st disk to add as “boot” disk (same menu that had REFORMAT in). Do the same with the 2nd disk. You should now see two 1.000M bios_grub partitions created under USED DEVICES. The trick to setup a softRAID array is to create partitions for /boot, swap and / on each disk, but WITHOUT formatting them (and as such, there won’t be a mount point for now). Lets create the boot partition. “Add GPT Partition” on the 1st disk, give it a 1G size and choose to leave it unformatted. Do the same for the 2nd disk. Lets Create the swap partition. “Add GPT Partition” on the 1st disk, give it the same or half the size of your RAM and choose to leave it unformatted. Do the same for the 2nd disk. Lets create the root partition. “Add GPT Partition” on the 1st disk, do not set a size (so it uses all available) and choose to leave it unformatted as with all the other partitions you created so far. Do the same for the 2nd disk. Now click on “Create software RAID (md)” under AVAILABLE DEVICES. We’ll create the first softRAID partition (md0) by selecting the two 1G partitions. Click create. Repeat the process for md1 and select the two swap partitions created earlier. Click create. Repeat the process for md2 and select the two root partitions. Click create. We now have 3 pairs of AVAILABLE DEVICES which will now format as the actual softRAID partitions. So select md0 and then “Add GPT Partition”, format as EXT4 and mount on /boot. Select md1 and then “Add GPT Partition”, format as SWAP. Select md2 and then “Add GPT Partition”, format as EXT4 and mount on /. All these mdX softRAID partitions will now appear under USED DEVICES and you are ready to proceed with Ubuntu’s installation. At the very bottom, you should now see “Done” enabled so hit it and proceed." }, { "title": "Ubuntu 22.04 WakeOnLan", "url": "/posts/wol/", "categories": "homelab, wol, wakeonlan, ubuntu", "tags": "homelab, wol, wakeonlan, ubuntu", "date": "2023-10-01 18:00:00 +0100", "snippet": "Wake On Lan…OS: Ubuntu 22.04 serverWake on lan has been around for a while, and while server grade hardware will have some sort of server management system, or of course the new pikvm which I have to get my hands on one day, I figure with the rise in fuel costs, lets utilise this wake on lan once and for all.Obviously, motherboard has to support it, and i’ll describe two methods i’ve used where one method will probably work outside of Ubuntu.NetplanSuper easy with the yaml file. Edit /etc/netplan/xyz.yaml# Let NetworkManager manage all devices on this systemnetwork: version: 2 ethernets: enp5s0: wakeonlan: true bridges: br0: interfaces: - enp5s0 dhcp4: yesBasically, just add the wakeonlan boolean to the interface which supports it… which by the way you can check with ethtool, but after adding wakeonlan to netplan…sudo netplan --debug generateif all goes well..sudo netplan --debug applyWhen checking with ethtool, use this command, replace with your interface.sudo ethtool enp5s0The output we’re looking for is…\tSupports Wake-on: pumbg\tWake-on: gIf you see the g, we are winning and it’s active. If it says ‘d’ then it has not worked.There is this other systemd method i’ve seen which i’ve also tried and tested as well.Create a systemd service… /etc/systemd/system/wol.service[Unit]Description=Enable Wake On Lan[Service]Type=oneshotExecStart=/usr/sbin/ethtool --change enp5s0 wol g[Install]WantedBy=basic.targetThen reload systemd so it sees the wol.service and enable it…sudo systemctl daeom-reloadsudo systemctl enable --now wol.serviceFingers crossed, your wakeonlan should be enabled if you have used either of these methods.Wake on Lan from Device…OK, with the server/desktop ready for wake-on-lan (wol) packets, on linux lets install wakeonlan.sudo apt install wakeonlanYou’ll need the nic mac address from the server which will be awakened. So on the server…ip aand look for something like this..2: enp5s0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel master br0 state UP group default qlen 1000 link/ether 48:e7:da:5c:98:f1 brd ff:ff:ff:ff:ff:ffNext to link/ether is the mac address.You can either send using wakeonlan without any args, or copy this to a file on your desktop (awakening device), and use wakeonlan with this file.wakeonlan -f wol.fileGood luck!" }, { "title": "ZFS Import Busy Dataset", "url": "/posts/zfs-busy-bug/", "categories": "homelab, lvm, zfs", "tags": "homelab, lvm, zfs, dataset, busy, import", "date": "2023-10-01 17:00:00 +0100", "snippet": "What happened?OS: Ubuntu 22.04 serverWell, after importing a zpool that was in use on another server (switching servers), I was unable to destroy the pool or dataset, faced with a busy dataset/pool error. I did notice something however, which helped with the trail.Even though there was no mounts, it was busy, and i believe it was LVM making use of it…zd32 230:32 0 60G 0 disk├─zd32p1 230:33 0 1M 0 part├─zd32p2 230:34 0 1G 0 part└─zd32p3 230:35 0 59G 0 part └─ubuntu--vg-ubuntu--lv 253:0 0 29.5G 0 lvmSo on my google search, i found someone who used lvm filter strings…Destroy ZFS PoolIf you edit the /etc/lvm/lvm.conf and if in vim, search for global_filter, you’ll find the section quick enough to place this custom filter. ### CUSTOM START filter = [ \"r|^/dev/zd*|\" ] global_filter = [ \"r|^/dev/zd*|\" ] ### CUSTOM ENDAfter this, update initramfs.update-initramfs -uAnd reboot.I only did this to destroy the ZFS dataset and pool. Afterwards I commented out the custom filter just to make sure it didn’t interfere with anything, but something worth remembering.zfs destroy -r fastZFSzpool destroy fastZFS" }, { "title": "Quick Proxy Setup for LXD", "url": "/posts/lxd-proxy-device/", "categories": "homelab, lxd, networking, proxy", "tags": "homelab, lxd, networking, proxy", "date": "2023-09-17 17:00:00 +0100", "snippet": "The why?So I don’t forget is the main reason on this occasion. I have been playing with the idea of setting up an LXD container for code-server, but with a machine with enough grunt and doesn’t zap too much power at idle. Queue in my laptop with a ryzen 5800h.Problem with the laptop? Yes. I don’t have ethernet for it, so I cant bridge a network with my wifi the way i normally would. So, i’ve learned about this proxy device trick instead.# On the host...# make sure you're on local if using laptop like melxc remote list# Then launch an ubuntu container and enter with bashlxc launch ubuntu:22.04 code-serverlxc exec code-server bash# Inside the container...# Replace ${VERSION} with the version you're installingapt update &amp;&amp; apt upgrade -y# Grab this version from their github and set the version variable.VERSION=4.16.1wget https://github.com/coder/code-server/releases/download/v${VERSION}/code-serer_${VERSION}_amd64.debdpkg -i code-server_${VERSION}_amd64.deb# Create a user for code-server. I'm adding to sudo, but you probably don't have to.useradd -m -s /bin/bash -G sudo jonnypasswd jonnysu jonny# Enable the service for jonnysudo systemctl enable --now code-server@$USER# Grab the password set for your user in their home directorycat .config/code-server/config.yamlsudo reboot#### On The hostlxc config device add code-server codeserverPort proxy listen=tcp:0.0.0.0:8080 connect=tcp:127.0.0.1:8080On your laptop, you can access via localhost:8080, but should be discoverable with your laptop ip address on port 8080 also.Obviously, you’ll want firewall rules in place here, but this will get you up and running." }, { "title": "Backup/Copy LXD container to another host", "url": "/posts/backup-lxd/", "categories": "homelab, lxd, backup, copy, hosts, servers", "tags": "homelab, lxd, backup, hosts", "date": "2023-09-17 17:00:00 +0100", "snippet": "The why?Although I run LXD in clusters, my clusters are generally low powered that don’t need much compute.So i do run some services that require compute, on my laptop or desktop in LXD, and here I will show a simple way to copy over the LXD container to another host.First you need to remote add and get a token, so in this example, lets assume my laptop is running a container and I want that container backed up.On my laptop…I will generate a token.lxc config trust addOn my desktop…I need to add my laptop..lxc remote add laptopWhen you paste in the token, no text will be displayed in the terminal, so don’t be alarmed. Just paste it in when prompted and press enter. Now remote switch to the laptop from the desktop.lxc remote switch laptopAnd now Copy the container over from the desktop…lxc copy container-name local:container-name-backup --verboseYou can then change back your remote to local on the desktop if you with…lxc remote switch localHopefully helps someone… it’ll help remind me in future… my memory needs help sometimes…" }, { "title": "MakeMKV install using snap", "url": "/posts/snap-makemkv/", "categories": "homelab, archive, media, HTPC", "tags": "home, archive, blu-ray, dvd, media, htpc", "date": "2023-08-26 13:00:00 +0100", "snippet": "The why?This is a useful programme for archiving collections of your blu-rays or DVDs, and for using with applications like jellyfin, plex etc.Very Easy Instructions using snapsThe latest/stable version is too old, so you need to install from latest/edge.sudo snap install makemkv --channel=latest/edgeAnd then all that is left to do is allow snap access to your dvd/blu-ray drivesudo snap connect makemkv:optical-write :optical-driveTo complete the installation, makemkv offer a free beta license, or you can support their cause. Link below…MakeMKV-Beta-License" }, { "title": "High Availability with KeepaliveD-Docker-LXD", "url": "/posts/docker-lxd-keepalive/", "categories": "homelab, Docker, keepalived, lxd, bash, high availability", "tags": "homelab, Docker, keepalived, lxd, bash, high availability", "date": "2023-04-28 13:00:00 +0100", "snippet": "Docker-LXD-KeepaliveDThis should be a mostly straight forward setup. There are plenty other ways to configure, this probably the easiest.Also, this would work better if you are in an LXD cluster, which is out of scope for this, but will add a short section about it further down.Also also, instead of a network share to synchronize the containers, you should probably use microceph with LXD (microcloud), which would also handle migration much better, but for now, i’ll go with network share, and will probably add a bit with microceph later - i’ve not tried it with docker, so we’ll see how that goes :) .Git RepoFirst of all, clone the git repo…git clone https://github.com/jonnypeace/docker-lxd-keepalived.gitSetting up an LXD container to use with dockerDocker cant run on zfslxc storage list # you'll see storage pools here.Create a pool called docker with btrfslxc storage create docker btrfsLaunch an ubuntu container called fast-ubuntulxc launch images:ubuntu/22.04 fast-ubuntuCreate storage volume for fast-ubuntulxc storage volume create docker fast-ubuntuAdd storage from storage pool docker created earlier to fast-ubuntu, and use it for /var/lib/dockerlxc config device add fast-ubuntu docker disk pool=docker source=fast-ubuntu path=/var/lib/dockerPoke some holes so docker can run inside the LXD containerslxc config set fast-ubuntu security.nesting=true security.syscalls.intercept.mknod=true security.syscalls.intercept.setxattr=trueRestart containerlxc restart fast-ubuntuEnter container and run bashlxc exec fast-ubuntu bashInstall docker (Worth checking dockers website in case this procedure changes) and install docker-compose &amp; keepalivedapt update &amp;&amp; apt upgrade -ysudo apt-get updatesudo apt-get install \\ ca-certificates \\ curl \\ gnupg \\ lsb-releasemkdir -p /etc/apt/keyringscurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpgecho \\\"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\$(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/nullsudo apt-get updatesudo apt-get install docker-ce docker-ce-cli containerd.io docker-compose-plugin docker-compose keepalivedTest docker installdocker run -it ubuntu bash # run exit or ctrl+D to exit - you'll want to remove this ubuntu image/container probably.List containers and grab ip.lxc list fast-ubuntu # for ipAdd network share - if you use this feature as per cron job below, adjust as necessary for both# add network shares mounted on host# Source is the host mount# path is the mount inside the lxd container# network-share is the name of the devicelxc config device add fast-ubuntu network-share disk source=/mnt/shares/ path=/mnt/dockerYou’ll need two of these containers to run keepalived and since its high availability and LXD clusters…lxc copy fast-ubuntu fast-ubuntu-slave --target server-nameIf you’re just testing on a local machine, remove the –target server-name part of the above commandmonitor.shThis is used by keepalived to keep track of a service on your network… in my example, i’ve used portainer as an exampleYou’ll want to modify this for your own subnet…ka_host_ip='10.10.200.69 9443' # EDIT to suit subnetThis ip will be the same (virtual) IP that is set in your keepalived config filesYou’ll also want to edit this array for container names - as per docker-compose files.cont_arr[1]='portainer' # This will also be used as my keepalived monitor containercont_arr[2]='freshrss'cont_arr[3]='vaultwarden'cont_arr[4]='watchtower'#cont_arr[5]=''#cont_arr[6]=''#cont_arr[7]=''Lastly for this script, the directory of /docker(/container) is assumed for all your docker-compose files.So you’ll want to modify as necessary. If you have docker-compose files scattered everywhere, this script won’t work.# root docker directory where containers located, following a pattern like so...# /docker/portainer, /docker/freshrss etc adjust as necessary.dock_dir='/docker'Master &amp; Slave keepalived config (obviously this is for a pair of servers).In your master server, copy etc.keepalived.master.conf file into /etc/keepalived/keepalived.confFor your slave server, copy etc.keepalived.slave.conf file into /etc/keepalived/keepalived.confAll that really needs modified here is this line, and ensure same virtual ip as in your monitor script. 10.10.200.69 # EDIT to suit subnetLastly, you’ll want to enable and start keepalivedsystemctl enable --now keepalivedIf you runsystemctl status keepalivedThe master will show…Entering MASTER STATEThe slave will show…Entering BACKUP STATE(VI_2) Changing effective priority from 80 to 81Or something along those lones, depending probably whether the slave was a master to begin with. The main takeaway is that one will be in master state, and the other in backup state.docker-update.shI will probably update this to make it more fancy, but for now, it’s a simple brace expansion, test directory, change to directory, and pull new image and start container.Again, this is assuming a root directory of /docker/containers….So modify as necessary.cron jobsAgain, this should be mostly self explanatory. I’ve shown how often i would run the monitor script, this means there should only really be around 1minute of downtime, give or take, depending if theres a docker image update.So edit this for the location of the keepalived monitor script.Monitors every minute for keepalived*/1 * * * * /home/user/monitor.shI’ve also shown a short rsync command to keep containers in sync over a mounted share, so you’ll want both servers to have the same mount if you use the same command as I.Hope this helps, feel free to let me know if there are any issues." }, { "title": "Automate IP blaclists with IPtables and IPset", "url": "/posts/abuse-ip/", "categories": "homelab, iptables, ipset, automate, firewall, bash", "tags": "iptables, ipset, security, firewall", "date": "2023-04-28 13:00:00 +0100", "snippet": "The why..Gather data from abuse IP database and update firewall rules using ipsetThis script is fairly recent, so might need some modifications.Dependencies….apt install ipset-persistent netfilter-persistent iptables-persistent iptables sed jq ipset fzf curlI’ve set this up to work alongside crowdsec, so you get another way of veriying your traffic.I’ve only tested this on servers with Ubuntu 22.04, might need modifications, especially for package checks at beginning of the script.setup ipset and iptablesRequires ipset list, 2 week timeout included…ipset create myset hash:ip timeout 1209600ipset create myset6 hash:net family inet6 timeout 1209600Requires iptables ruleiptables -I INPUT -m set --match-set myset src -j DROPip6tables -I INPUT -m set --match-set myset6 src -j DROPAlso requires to make sure iptables is persistent after new rulesudo dpkg-reconfigure iptables-persistentThis script..Githuib link..git clone https://github.com/jonnypeace/ip-abuse-bash.gitYou will also need an API key from abuseipdb.com and sign up for an account.Store your api.key in the same folder as this script, with filename api.keyThis script will save files in it’s current directory, so maybe best keeping in this git directoryYou will want to modify the variable for the location you are using for this repo.ip_file_path=\"$HOME/git/ip-abuse-bash\"Run this for some help…./abuseIP.sh -hOutput:On first run you will need the 10,000 IPs, so run with -g flag.Option -c : check if ip is in databse, run ./abuseIP.sh -c 123.12.123.12Option -g : Get 10,000 IP list from database, no further args requiredOption -v : adds verbose to ip checking -c option onlyOption -f : adds fuzzy file selection to adding ip rules to ipset. Any file ending in .ip and in this git directory.Option -A : adds automation script for set files to update, any file ending in .ip and in this git directory.Option -u : Check ufw logs and update IPSET - DO NOT USE -v VERBOSE!!Option -h : Help tips :DThere are not a lot of options, and for automation in crontab, i run…0 */3 * * * /home/jonny/ip-abuse-bash/abuseIP.sh -gA &gt; /dev/null &amp;&amp; echo 'database updated and added to ipset'*/30 * * * * /home/jonny/ip-abuse-bash/abuseIP.sh -u &gt; /dev/null &amp;&amp; echo 'ufw log checked and ips added to ipset'If you log more UFW firewall traffic with something like this…ufw limit log 443/tcpYou might want to adjust the crontab timescale or upgrade your abuse IP database subscription :)" }, { "title": "Automate UFW with Nginx and TCP wrapper", "url": "/posts/ufw-nginx/", "categories": "homelab, firewall, UFW, Nginx, Security", "tags": "Firewall, UFW, Nginx, Security, hosts.allow", "date": "2023-04-02 12:00:00 +0100", "snippet": "The why?This is a shell script that I do use quite frequently, since it’s been run in a cron job to check for host lookup ip changes (utilizing dynamic DNS).This saves users from being locked out if their home ip address changes, also helps keep a stronger firewall.This basic example will allow two people to access SSH and HTTPS ports.. This works so long as those two people are allowed access on those ports, so this script would probably need modified to accomodate, or just use seperate scripts for groups of individuals.The only section of this script to adjust are the two arrays, one for ports and the other is clients, at the top of the script. If you want to add more, just follow the same format with ports and clients. Comment out or delete sections with nginx or hosts.allow if not required.If using hosts.deny, this script will accomdate a hosts.allow, but obviously this could be modified also. If you want to use this TCP wrapper, put a ‘sshd: ALL’ or something along those lines which suits your needs in your hosts.deny.If using the nginx side, place this key in the server block of your nginx config. #SED-MATCH-LEAVEI could have used a substitute option for sed up to a point, but at least with this method, it will help with the set up of a newly created config or new user.Also if using the nginx config like me, and you don’t need the world wide web on your server, place a deny all; in the server block underneath the allowed ip’s.#!/bin/bash#allow a dyndns name through your firewall. The idea is to set up a crontab#to look up the dns hostname so you always have access to the server, via#your port and protocol of choice. I use something like this to keep a VPN#tunnel open to a server.#for manual entry, syntax in terminal for UFW....#./script.sh ip port proto, i.e.#./script.sh 123.12.123.12 22 tcpdeclare -A ports# Update ports to cycle through your firewallports[ssh]=12345ports[https]=443#ports[]=declare -A clients# Check hosts associated with clientclients[jamie]=$(host homeIPaddress1.ddns.net | cut -f4 -d' ')clients[holly]=$(host homeIPaddress2.ddns.net | cut -f4 -d' ')#clients[]=# nginx config filenginx_conf='/etc/nginx/conf.d/web.conf'# Check if any ufw rulesif [[ $(sudo ufw status | wc -l) == 1 ]]then append=Truefimkdir -p \"$HOME\"/logstimelog=\"$HOME\"/logs/dyndnstime.lognow=$(printf '%(%d-%m-%Y %H:%M:%S)T\\n')function sed-nginx { # Update an nginx config if you have one? Test on a dummy nginx config beforehand. # You might also want to change a subnet instead, replace or leave out entirely # An example of append (Placing that comment '#SED-MATCH-LEAVE' somewhere in your nginx config file (server block)) # I'm using \\\\\\t to get a tab space in the conf, might need adjusted sudo sed -i \"/#SED-MATCH-LEAVE/a \\\\\\tallow ${clients[$client]};\" \"$nginx_conf\"}# adds new UFW firewall rulesfunction add-new-rules { for port in \"${ports[@]}\"; do if [[ $append == True ]]; then sudo ufw allow from \"${clients[$client]}\" to any port \"$port\" proto tcp else # The reason for inserting new rules, is to keep them above any rate limiting rules which might interfere with the users. I've chosen 1 to keep above all rules. sudo ufw insert 1 allow from \"${clients[$client]}\" to any port \"$port\" proto tcp fi done}# If no commandline options...if [[ -z \"$1\" ]]then for client in \"${!clients[@]}\" do # If no ip found from host, continue to next host, otherwise hosts.allow may fill with junk if [[ ! ${clients[$client]} =~ [0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3} ]]; then echo \"No ip found from ${client}, moving to next client\" continue fi logfile=\"${HOME}/logs/${client}.ip.log\" if [[ ! -f \"$logfile\" ]] then # data will be stored in the users $HOME directory and not the root directory. # add new UFW rules add-new-rules # Update an nginx config if you have one? Test on a dummy nginx config beforehand. # Remove or comment out line below if you don't need this sed-nginx # hosts allow file update echo \"ALL: ${clients[$client]} # Home\" | sudo tee -a /etc/hosts.allow echo \"${clients[$client]}\" &gt; \"$logfile\" echo \"New host time log created\" &gt; \"$timelog\" else old_ip=$(&lt; \"$logfile\") if [[ ${clients[$client]} == \"$old_ip\" ]] then echo \"$client IP address has not changed\" echo \"$now : $client : log running every hour, no changes\" &gt;&gt; \"$timelog\" else # Remove old IPs from client num_rules=$(sudo ufw status numbered | grep -c \"$old_ip\") for (( a=1; a&lt;=num_rules; a++ )); do first_rule=$(sudo ufw status numbered | grep \"$old_ip\" | sed -En \"1s/.*([0-9]+)\\].*/\\1/p\") echo \"y\" | sudo ufw delete \"$first_rule\" done # Clean up hosts.allow file sudo sed -i \"/$old_ip/d\" /etc/hosts.allow # Clean up nginx conf sudo sed -i \"/$old_ip/d\" \"$nginx_conf\" # Add new rules add-new-rules # Update an nginx config if you have one? Test on a dummy nginx config beforehand. You might change to many ip addresses with this command. # Remove line below if you don't need this sed-nginx # update hosts allow file echo \"ALL: ${clients[$client]} # Home\" | sudo tee -a /etc/hosts.allow echo \"${clients[$client]}\" &gt; \"$logfile\" echo \"$client IP has been updated\" echo \"$now : $client New IP updated\" &gt;&gt; \"$timelog\" fi fi doneelse # Sense checking the commandline entries if [[ ! $1 =~ [0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3} ]]; then echo \"$1 is not a valid ip address\" exit 1 fi if [[ ! $2 =~ [0-9]{1,5} ]]; then echo \"$2 is not a valid port number\" exit 1 fi if [[ ! $3 =~ ^tcp$|^udp$ ]]; then echo \"$3 is not a valid protocol, use tcp or udp\" exit 1 fi if [[ $append == True ]]; then sudo ufw allow from \"$1\" to any port \"$2\" proto \"$3\" else sudo ufw insert 1 allow from \"$1\" to any port \"$2\" proto \"$3\" fi echo \"ALL: $1 # Home\" | sudo tee -a /etc/hosts.allow # Update an nginx config if you have one? Test on a dummy nginx config beforehand. # Remove line below if you don't need this # An example of append (Placing that comment '#SED-MATCH-LEAVE' somewhere in your nginx config file (server block)) # I'm using \\\\\\t to get a tab space in the conf, might need adjusted sudo sed -i \"/#SED-MATCH-LEAVE/a \\\\\\tallow $1;\" \"$nginx_conf\" fi" }, { "title": "Ansible / Bash - Nextcloud Deployment - LXD", "url": "/posts/ans-nc-lxd/", "categories": "bash, ansible, scripting, ubuntu, lxd, nextcloud, mariadb, backup", "tags": "bash, ansible, lxd, nextcloud", "date": "2022-12-10 13:00:00 +0000", "snippet": "Easy Nextcloud Deployment using bash with ansible.There are probably ways to make this work with ansible only, but I thought I would use a mix of skills, and it seems to work with just a few configuration adjustments.If not done so already, perhaps explore the idea of an LXD host, as i’ve written this for remote LXD hosts. You could just as easily modify this for a local LXD host.Since this is taking place over LXD, there’s no need to use SSH. You can use lxc remote switch ‘host’ and this will work. This means you don’t need ansible vault.I have used docker for such instances, but I quite like what Ubuntu are doing right now with snaps, LXD, microceph and microcloud. If you are now aware of these applications, I encourage you to have a look.GithubClone my repo for this…git clone https://github.com/jonnypeace/ansible-lxd-nextcloud.githosts.yamlThe hosts.yaml file needs updated where the comments are, but as you can see you could modify this for local use.---all: vars: ansible_connection: lxd ansible_user: root ansible_become: no children: local: vars: ansible_lxd_remote: local ansible_lxd_project: homelab hosts: lxd-host: # Adjust to same name as anible_lxd_remote vars: ansible_lxd_remote: lxd-host # Enter your lxd remote host, check with 'lxc remote list' hosts: new-container: # This will be the name of the LXD container, so will need adjusted.backup.shThis is a simple backup and restore script for mariadb and nextcloud. The ansible playbook will use this to restore an old/existing nextcloud, but you will be prompted as to whether this is required. You could always use it to backup and restore day-to-day.However, this section at the top of the script needs modified to suit your needs.nc_backup='/mnt/nextcloud/backup/nextcloud.tar.bz2'nextcloud_dir='/var/www/domain.com/'db_pass=$(&lt; .db-password.txt)db_backup='/mnt/nextcloud/backup' # this is a directoryConfigThis should hopefully be self explanatory, comments are already provided in the script to help guide.# Container Namecontainer-name=nc-new# Shared directory/external storage for nextcloud on LXD hosthost-share=/mnt/shares/nextcloud# Where to mount the directory/external storage in the containercont-mount=/mnt/nextcloud# For LXD... a name for the device being added to LXDshare-name=nextcloud# Nextcloud backup, used for restoring an existing nextcloud installationnextcloud-source=/mnt/shares/nextcloud/backup/nextcloud.tar.bz2# Database backup, used for restoring the database from an existing nextcloud installation - # Will only work with the same nextcloud instance described above.tar-db=/mnt/shares/nextcloud/backup/nextcloud-sqlbkp_20221126.bak# This will be used in your /var/www directorywww-dir=domain.com# This will be used in your virtual hosts file.domain=domain.com# This is for your apache2 server.virtual-hosts-file=domain.com.confnodes_update.yamlThis is used to update nodes. Since this is a baremetal nextcloud/mariadb, a simple apt package manager will update most of the container. Nextcloud will need manual updates either via the nextcloud admin page, or something a little more manual.---- name: Update and Upgrade LXD cluster nodes hosts: nc-new tasks: - name: Update and upgrade apt packages apt: upgrade: yes update_cache: yes cache_valid_time: 86400.db-password.txtI would create this file and use this for your mariadb nextcloud password. Once the password has been added, run…chmod 400 .db-password.txttemplate.yamlYou shouldn’t need to make any modifications, the script will make the modifications covered in the config files, but feel free to make your own improvements or tweaks that you require. This template will be used to create your nextcloud container playbook file.setup.shOk, once all config files are ready, this script will guide you through the setup../setup.shWireguard ScriptI have left this script in, as originally I would reverse proxy with a VPS directly from nextcloud, but now I put nextcloud behind a second proxy and this proxy takes care of the wireguard traffic (much easier). I will eventually write something with how I do this.ExtrasIf you want to upload files larger than 10MB (where I was having problems), chunk size issues may arise with Nextcloud. Try this inside your lxd container…sudo -u www-data php --define apc.enable_cli=1 occ config:app:set files max_chunk_size --value 0" }, { "title": "Ubuntu Router", "url": "/posts/Ubuntu-Router/", "categories": "homelab, project, routing", "tags": "servers, homelab, bash, routing, firewall, ubuntu", "date": "2022-08-06 13:30:00 +0100", "snippet": "Ubuntu RouterUpdate 26-04-2023Around 3 months ago I upgraded to a N5105 5 port 2.5gb router using the exact same set up, after upgrading my LAN to full 1gbps fibre. I never stumbled across any issues with the raspberry pi setup, but will issue a blog update with new setup :)I’ve been using my raspberry pi4 with 2 extra USB3 to LAN Gb adapters for a few months and it has felt quite stable for a routing device. My ISP speed is 65Mbps Download and 18Mbps Upload, and with the lack of AES, i’ve not attempted to do VPN traffic, but i have tested it with openelec, and it was able to push through my ISP download speed, but it might struggle if you have a better internet connection.So, the Ubuntu routerI’ve decided to stay with the UFW firewall, but i have thought about trying firewalld.. maybe for another day. With the type of routing I required, I decided to make a bridged network, and hence the two extra USB ethernet ports.Packages installed: isc-dhcp-server cockpitSo lets beginNetplanCheck which interfaces are available…ip aThe three interfaces i’m interested are the built in ethernet and 2 x USB ethernet. eth0 - built-in ethernet enxa0cec8c0b0e2 - usb ethernet enx000ec6dab3a7 -usb etherneteth0 will be my WANenx000ec6dab3a7 &amp; enxa0cec8c0b0e2 will be my bridge network br0.10Firstly, lets create the bridge network using netplan…network: ethernets: eth0: addresses: [192.168.1.10/24] routes: - to: default via: 192.168.1.1 nameservers: addresses: [9.9.9.9, 149.112.112.112] enx000ec6dab3a7: {} enxa0cec8c0b0e2: {} bridges: br0.10: addresses: [10.10.10.1/24] nameservers: addresses: [9.9.9.9, 149.112.112.112] interfaces: - enxa0cec8c0b0e2 - enx000ec6dab3a7 version: 2 renderer: NetworkManagereth0Addresses is the ip address for the ubuntu router, which is connected to my ISP router (for the WAN), and the ISP router supplies this LAN ip.For eth0 default routes via… this is my router ip address 192.168.1.1.For eth0 nameservers, i’ve just included quad 9’s IP address, but update with your own preference.enx000ec6dab3a7: {} - This basically identifies the interface for later useenxa0cec8c0b0e2: {} - This basically identifies the interface for later useBridgesbr0.10 is the interface i’ll be using for my LAN, using the ip address10.10.10.10.1 will be the ubuntu lan ip which devices connect to.Nameservers is the same idea as the WAN.Includes both interfaces for the bridgerenderer - i added this in for NetworkManager, so i could use cockpit with this setup.isc-dhcp-serversudo apt install isc-dhcp-serverNow lets edit this file /etc/default/isc-dhcp-serverat the bottom include these two linesINTERFACESv4=\"br0.10\"INTERFACESv6=\"\"/etc/dhcp/dhcpd.confNow lets edit this file /etc/dhcp/dhcpd.confdefault-lease-time 43200;max-lease-time 86400;authoritative;subnet 10.10.10.0 netmask 255.255.255.0 { option domain-name \"home.lan\"; option routers 10.10.10.1; option subnet-mask 255.255.255.0; option broadcast-address 10.10.10.255; range 10.10.10.100 10.10.10.200; option domain-name-servers 9.9.9.9;}Lease times are something a DHCP server will renew, and these are the times i’ve decided on.This sets up a 10.10.10.1/24 LAN network for your devices to connect to with DHCP.The range of 10.10.10.100-10.10.10.200 is for the dhcp server. This reserves the 10.10.10.2-10.10.10.99 &amp; 10.10.10.201+ range for static IPsLooking at ubuntu’s documentation on this, it looks like you can assign static IP’s using the MAC addresses in this config file.Ubuntu - isc-dhcp-server linkAll my static IP’s are done on the host, so it’s not something i’ve centralised like this.dhcp leasesYou can view your DHCP leases here…less /var/lib/dhcp/dhcpd.leasesUFWOk, this will depend a little on your services, but this is where i’ve went with it.sudo ufw limit from 10.10.10.0/24 to any port 22 proto tcpsudo ufw allow from 10.10.10.0/24 to any port 67 proto udpsudo ufw allow from 10.10.10.0/24 to any port 68 proto udpsudo ufw allow from 10.10.10.0/24 to any port 53 proto tcpsudo ufw allow from 10.10.10.0/24 to any port 53 proto udpsudo ufw allow from 10.10.10.0/24 to any port 9090 proto tcpsudo ufw route allow in on br0.10 out on eth0I’ve allowed ssh (with a rate limit) access on the LAN, DHCP (67,68), DNS(53), and cockpit(9090).I’m allowing traffic to be forwarded from the bridge, to the WAN.We also need to edit /etc/ufw/sysctl.conf and uncomment this..net.ipv4.ip_forward=1Lastly for the firewall, we need a NAT masquerade.Lets edit the /etc/ufw/before.rules and include these lines before the filter rules.# nat Table rules*nat:POSTROUTING ACCEPT [0:0]# Forward traffic from br0.10 through eth0.-A POSTROUTING -s 10.10.10.0/24 -o eth0 -j MASQUERADE# don't delete the 'COMMIT' line or these nat table rules won't be processedCOMMITDirectly under the MASQUERADE rule, the rule for that table must have COMMIT. This is the case for each table in these rules.We should be able to apply our netplan now..sudo netplan --debug generatesudo netplan generatesudo netplan applyWe should be able to apply our firewall rules now as wellsudo ufw disable &amp;&amp; sudo ufw enableufw reload might work, but the documentation from Ubuntu states the above. command.If you want a birds eye view of your router, cockpit is great, and if you’ve configured your firewall like mine, you should be able to connect to it via port 9090So i would install cockpit…sudo apt install cockpitFor the UFW firewall, check out Ubuntu’s website, they might cover some commands you’ll find useful.Ubuntu UFW DocsVLANsUpdated: 07/11/2022Ok, I wanted to write about this earlier in the year, but my managed switch died on me, and I’ve only just got round to replacing it.FYI, I am using a TP-Link smart managed switch here, nothing too fancy but this is the model number TL-SG1016DE. All I need is a little network segregation and this fits the bill for a reasonable price. No high speeds in this house, but gigabit LAN has been good to me over the years.Due to me self hosting some services, I wanted to keep the self hosted network separate from my safer home network, and hence the vlan. I am going to continue with the bridge netplan discussed earlier, and include one VLAN in this configuration. This vlan network consists of an LXD cluster and some other goodies, but for container creation, i need DHCP as i’m using macVLAN with the LXD cluster, so in this section i’ll have to include something for the VLAN DHCP server, so lets begin…VLAN - Netplan YAMLnetwork: ethernets: eth0: addresses: [192.168.1.10/24] routes: - to: default via: 192.168.1.1 nameservers: addresses: [9.9.9.9, 149.112.112.112] enx000ec6dab3a7: {} enxa0cec8c0b0e2: {} bridges: br0.10: addresses: - 10.10.10.1/24] nameservers: addresses: [9.9.9.9, 149.112.112.112] interfaces: - enxa0cec8c0b0e2 - enx000ec6dab3a7 vlans: vlan2: id: 2 link: br0.10 addresses: [10.10.20.1/24] version: 2 renderer: NetworkManager/etc/ufw/before.rulesThis file now has this inclusion of the VLAN (10.10.20.0/24)masquerade..# nat Table rules*nat:POSTROUTING ACCEPT [0:0]# Forward traffic from vlan2 &amp; br0.10 through a NAT on eth0.-A POSTROUTING -s 10.10.10.0/24 -o eth0 -j MASQUERADE-A POSTROUTING -s 10.10.20.0/24 -o eth0 -j MASQUERADE# don't delete the 'COMMIT' line or these nat table rules won't be processedCOMMITOk this bit will be dependant on the whether you want one way traffic or any sort of communication between your VLAN and safer home network, and i do, so i’ll include them and explain.# When you find the rules below....# Don't delete these required lines, otherwise there will be errors*filter:ufw-before-input - [0:0]:ufw-before-output - [0:0]:ufw-before-forward - [0:0]:ufw-not-local - [0:0]# End required lines# ENTER FORWARD RULES NOW...#### MY forward# vlan2 to WAN - this could be included with the standard UFW command, but since i'm here (a few of these rules could probably be applied on the commandline, but this was easier to test the rules)...-A ufw-before-forward -i vlan2 -o eth0 -j ACCEPT## Firewall rules between vlan2 and specific device.# NFS share for jellyfin - i've had to set it up this way, so the jellyfin container can mount the NFS share. # Funnily enough, I am using firewalld on this server, so i added the jellyfin to the public zone where only NFS is allowed. # Also, with NFS, i have provided these mount paths with read only access from /etc/exports, just to harden security. Jellyfin doesn't need write access to them.-A ufw-before-forward -s 10.10.10.40/32 -o vlan2 -j ACCEPT-A ufw-before-forward -i vlan2 -d 10.10.10.40/32 -j ACCEPT# Leave this section at the bottom of the MY forward rules. This uses conntrack to prevent vlan2 from initiating new connections with my phone or laptop that i've let through the guard.-A ufw-before-forward -o vlan2 -j ACCEPT-A ufw-before-forward -i vlan2 -m state ! --state NEW -j ACCEPT-A ufw-before-forward -i vlan2 -m state --state NEW -j REJECTOnce those rules are added, remember to…sudo ufw reloadUFW rulesOk, now some standard DHCP and DNS access rules. It’s worth pointing out. I’m using DNS 9.9.9.9 etc in my netplan example, but the reality is i’m using pihole, or i could use bind.sudo ufw allow from 10.10.20.0/24 to any port 67 proto udpsudo ufw allow from 10.10.20.0/24 to any port 68 proto udpsudo ufw allow from 10.10.20.0/24 to any port 53 proto tcpsudo ufw allow from 10.10.20.0/24 to any port 53 proto udpVLAN - DHCP ServerOk, time to add the vlan2 to the DHCP server./etc/dhcp/dhcpd.confdefault-lease-time 43200;max-lease-time 86400;authoritative;## vlan2subnet 10.10.20.0 netmask 255.255.255.0 { option domain-name \"vlan2.lan\"; option routers 10.10.20.1; option subnet-mask 255.255.255.0; option broadcast-address 10.10.20.255; range 10.10.20.50 10.10.20.100; option domain-name-servers 9.9.9.9;}## home.lansubnet 10.10.10.0 netmask 255.255.255.0 { option domain-name \"home.lan\"; option routers 10.10.10.1; option subnet-mask 255.255.255.0; option broadcast-address 10.10.10.255; range 10.10.10.100 10.10.10.200; option domain-name-servers 9.9.9.9;}/etc/default/isc-dhcp-serverINTERFACESv4=\"br0.10 vlan2\"INTERFACESv6=\"\"If there is anything that could be improved with this setup, please get in touch. I never included stateful rules with the ufw forward, simply because ufw-before-forward rules have this included by default…# quickly process packets for which we already have a connection-A ufw-before-input -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT-A ufw-before-output -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT-A ufw-before-forward -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPTAnd if you have set this up like I have, and scan iptables…sudo iptables -vL | lessAnd look for…Chain ufw-before-forward (1 references) pkts bytes target prot opt in out source destination 1425K 78M ACCEPT all -- vlan2 eth0 anywhere anywhere 0 0 ACCEPT all -- any vlan2 10.10.10.185 anywhere 181 12442 ACCEPT all -- any vlan2 10.10.10.175 anywhere 20486 1846K ACCEPT all -- any vlan2 nfs.home anywhere 27170 2328K ACCEPT all -- vlan2 any anywhere nfs.home 3276K 4648M ACCEPT all -- any vlan2 anywhere anywhere 183K 81M ACCEPT all -- vlan2 any anywhere anywhere ! state NEW 5 372 REJECT all -- vlan2 any anywhere anywhere state NEW reject-with icmp-port-unreachable 43M 53G ACCEPT all -- any any anywhere anywhere ctstate RELATED,ESTABLISHED 0 0 ACCEPT icmp -- any any anywhere anywhere icmp destination-unreachable 0 0 ACCEPT icmp -- any any anywhere anywhere icmp time-exceeded 0 0 ACCEPT icmp -- any any anywhere anywhere icmp parameter-problem 4836 342K ACCEPT icmp -- any any anywhere anywhere icmp echo-request59103 10M ufw-user-forward all -- any any anywhere anywhere You can see the ctstate RELATED,ESTABLISHED processes the bulk of the packets (53G), and with my understanding… this accomplishes a stateful rule for the before-forward traffic. There’s every chance i’ve missed or included something unnecessary, and I would be forever grateful if this was highlighted to me. Fingers crossed you have a new ubuntu router, with VLAN traffic.The Switch - TP Link TL-SG1016DEExamples for making this simple smart managed switch to work with vlans. Just remember to set the vlan ID to the same ID as on your ubuntu router.bridgevlan2In this example, i’ve removed ports 4, 6 and 8 on the vlan config to non members of vlan ID 1 (vlan id 1 is untagged so will work on bridge).I then added ports 4, 6 and 8 to the vlan id 2 untagged. Port 16 in this example is tagged, which is the port connecting to the ubuntu router. It will tag all untagged ports and send to the ubuntu router.default vlan-idThis section is so simple yet i overlooked it when testing. Everything defaults to vlan id 1 on initial setup. So when running vlans, you will need to set the default here. In this example I have set ports 4, 6 and 8 to default to vlan id 2. This is quite different from my old linksys/cisco switch, but not too different.Fingers crossed you have a functioning ubuntu router, and with vlans if you choose." }, { "title": "Simple Incremental Backup Script", "url": "/posts/inc-tar/", "categories": "homelab, project, backup", "tags": "tar, compress, backup, incremental", "date": "2022-06-10 19:00:00 +0100", "snippet": "update: 10-02-2024The Concept The backup script holds onto 2 weeks worth of DAILY incremental backups. This backup solution uses tar with gz compression or without compression with the -n flag. Once you’ve ensured that you are backing up all relevant files/directories, run on a crontab at a time which suits you. If you have made an error during testing, it is BEST TO REMOVE ALL THE TAR files (including the file.inc) and start over, because the file.inc won’t know what’s changes you’ve made. I’ve included a -h flag for help, which provides an example of backup and recovery. In the examples provided, i’ve named this script backup.sh.Why this and not use tar directly?Well… You could use tar directly, but the idea here is to manage a little more automation for tar.. Including: 2 weeks worth of DAILY backups always organized backups are incremental and automated therefore taking up less disk space. Easier to remember syntax for recovery. What use is a backup, if you cant recover. Compression and verbose enabled by default You can either copy &amp; paste the script directly from here, or copy from the repo.git clone https://github.com/jonnypeace/backups-tar.gzip.gitThe backup &amp; recovery script#!/bin/bash# Author: Jonny Peace#### If includes_file and dirfrom, then exit with error.if [[ $* =~ ' -b ' &amp;&amp; $* =~ ' -i ' ]]; then printf '\\033[91m%s\\033[0m\\n\\n' \"When using -i, do not use -b or errors occur. You may as well add all directories to the include file and start again\" exit 1fifunction sense_check { echo \"Destination Directory set: ${dirto:?'-d (destination) is not set. Exiting.'}\" backfile=\"${backfile:-backupfile}\"}function check_includes { [[ ! -f \"$includes_file\" ]] &amp;&amp; echo \"Error: Includes File (-i) is not an actual file ${includes_file:-Includes_File_Not_Set}\" &amp;&amp; exit 1 }function check_excludes { [[ ! -f \"$excludes_file\" ]] &amp;&amp; echo \"Error: Excludes File (-e) is not an actual file ${excludes_file:-Excludes_File_Not_Set}\" &amp;&amp; exit 1 }function check_dirbk { [[ ! -d \"$dirbk\" ]] &amp;&amp; echo \"Error: Destination directory '$dirbk' does not exist.\" &amp;&amp; exit 1}function check_backup_dir { [[ ! -d \"$dirfrom\" ]] &amp;&amp; echo \"Error: Backup Directory (-b) does not exist; ${dirfrom:-Backup_Dir_Not_Set}, exitting.\" &amp;&amp; exit 1}# This function will provide the tar.gz backup archive utilizing the incremental file for monitoring.function backup { sense_check mkdir -p \"$dirto\" # incremental file which keeps track of changes (no need to change this) incfile=file.inc # date for directory and .tgz file naming # format example 2023-05-28_23.10.05 # yyyy-mm-dd_h.m.s day=$(printf '%(%Y-%m-%d_%H.%M.%S)T\\n') # Enable nullglob to ensure arrays are empty if no matching files are found shopt -s nullglob #Check number of directories with week-ending, and count them dirnum=0 for dir in \"$dirto\"/*week-ending*/; do [[ -d \"$dir\" ]] &amp;&amp; ((dirnum++)) done # My aim is to keep two weeks of backups at all times. # If you want to adjust this, adjust the number 3 accordingly. # Example: 3 will keep 2 full weeks of dailing backups. backup_dirs=(\"$dirto\"/*week-ending*/) if [[ ${#backup_dirs[@]} -ge 3 ]]; then # The naming convention includes sortable date strings, # simply sorting the names alphabetically should suffice. # More so if this directory is only used by this script. oldest_dir=\"${backup_dirs[0]}\" echo \"Removing oldest directory: $oldest_dir\" rm -r \"$oldest_dir\" fi tar_files=(\"$dirto\"/*.tar \"$dirto\"/*.tar.gz \"$dirto\"/*.tgz) filenum=${#tar_files[@]} shopt -u nullglob # Once 7 .tgz are created, move them to a new week-ending directory # If run daily on cron job, this will be a weeks worth of incremental backups if [[ \"$filenum\" -ge 7 ]]; then arch_dir=\"${dirto}/week-ending-${day%_*}\" mkdir -p \"$arch_dir\" mv \"$dirto\"/*.tar* \"$arch_dir\" mv \"$dirto\"/\"$incfile\" \"$arch_dir\" fi # Create .tgz file. Ideally this will work in a cron job, and you'll get daily backups # to exclude a directory after the tar command, example --exclude='/home/user/folder' declare -a args if [[ \"${no_comp}\" ]]; then args=(\"-vc\" \"-g\" \"${dirto}/${incfile}\" \"-f\" \"${dirto}/${backfile}-${day}.tar\") else args=(\"-vcz\" \"-g\" \"${dirto}/${incfile}\" \"-f\" \"${dirto}/${backfile}-${day}.tar.gz\") fi if [[ \"${includes_file}\" ]]; then [[ ! -f \"${includes_file}\" ]] &amp;&amp; echo \"Includes File Not a File: ${includes_file}\" &amp;&amp; exit 1 args+=(\"-T\" \"${includes_file}\") else args+=(\"${dirfrom}\") fi if [[ \"${excludes_file}\" ]]; then [[ ! -f \"${excludes_file}\" ]] &amp;&amp; echo \"Excludes File Not a File: ${excludes_file}\" &amp;&amp; exit 1 args=(\"-X\" \"${excludes_file}\" \"${args[@]}\") fi tar \"${args[@]}\"}# This function will recover the data, and requires all tar files from the backup directory and the incremental file.# the -g /dev/null keeps tar happy.function recovery { sense_check (mkdir -p \"$dirto\" shopt -s nullglob shopt -s globstar for file in \"$dirbk\"/**/*.tar*; do tar -vx -g /dev/null -f \"$file\" -C \"$dirto\" done)}# This loop relies on the commandline flags so it knows which function to choose.# The reason for the if statements is to account for user input, and whether they include # a forward slash at the end.while getopts b:r:d:e:f:ni:h optdo case \"$opt\" in b) dirfrom=\"$OPTARG\" if [[ \"${dirfrom:0-1}\" == '/' ]] ; then dirfrom=\"${dirfrom::-1}\"; fi check_backup_dir;; r) dirbk=\"$OPTARG\" if [[ \"${dirbk:0-1}\" == '/' ]] ; then dirbk=\"${dirbk::-1}\"; fi check_dirbk ;; d) dirto=\"$OPTARG\" if [[ \"${dirto:0-1}\" == '/' ]] ; then dirto=\"${dirto::-1}\"; fi;; e) excludes_file=\"$OPTARG\" check_excludes ;; n) no_comp='true' ;; i) includes_file=\"$OPTARG\" check_includes;; f) backfile=\"$OPTARG\" ;; h) cat &lt;&lt; EOF backup.sh script for backup and recovery. Select -b for backup followed by backup directory Select -r for recovery followed by location of backup directory Select -d for destination followed by directory to restore or backup to Select -f for name of backup file Select -n for no compression Select -e for excludes file Select -i for includes file Select -h for this help DO NOT use -b WITH -i flag. If you want to add another directory or file, then add it to the includes file (-i) as there should be no need to use both. * Example for backup: ./backup.sh -d /mnt/NFS/backup/ -f filename -b $HOME/files/ * Example for restore: ./backup.sh -d $HOME/files/ -r /mnt/NFS/backup/ * Example of backup utilizing the excludes file: ./backup.sh -d /mnt/NFS/backup/ -f filename -e excludes.file -b $HOME/files/ * Example for no compression. Just add the -n flag, no further args required: ./backup.sh -d /mnt/NFS/backup/ -n -f filename -e excludes.file -b $HOME/files/ * Example for includes file (IMPORTANT: the -i flag cannot be used with -b ): ./backup.sh -d /mnt/NFS/backup/ -n -f filename -e excludes.file -i includes.fileEOF exit ;; *) echo 'Incorrect option selected, run ./backup.sh -h for help' exit esacdone[[ -z \"$dirfrom\" &amp;&amp; -z \"$includes_file\" ]] &amp;&amp; echo \"Required an option of -b backup directory or -i includes file, exitting...\" &amp;&amp; exit 1[[ -n \"$dirfrom\" || -n \"$includes_file\" ]] &amp;&amp; backup[[ -n \"$dirbk\" &amp;&amp; -n \"$dirto\" ]] &amp;&amp; recoveryPotential improvements Considering the use of –one-file-system as a default or option. Considering making the backup timeline more flexible than 7 increments per folder (2 weeks of daily backups)" }, { "title": "Welcome to my Homelab", "url": "/posts/Welcome/", "categories": "homelab, project", "tags": "servers, homelab, bash, python, gawk", "date": "2022-06-10 16:00:00 +0100", "snippet": "WelcomeHello and welcome to my homelab project siteWebsite project in progressHere I will be sharing some of my homelab projects and some scripts using mostly…. Ansible Bash Python Awk/Gawkecho \"This code block will be used for bash\"print(\"Welcome from a python codeblock\")Placeholder template for adding images..Chess 101" } ]
